{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ビジネス関連法案の RAG 検索 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import cohere\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aki/tech/ai-api-playground/env/lib/python3.14/site-packages/cohere/core/pydantic_utilities.py:13: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.datetime_parse import parse_date as parse_date\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "co = cohere.Client(os.environ['COHERE_APIKEY'])\n",
    "\n",
    "EGOV_SEARCH_URL = \"https://elaws.e-gov.go.jp/api/1/lawlists/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 2. e-Gov から法令情報を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "documents = [\n",
    "    {\n",
    "        'title': '昭和六十三年法律第百八号 消費税法',\n",
    "        'url': 'https://laws.e-gov.go.jp/api/1/lawdata/363AC0000000108', #https://laws.e-gov.go.jp/law/363AC0000000108'\n",
    "    },\n",
    "    #{\n",
    "    #    'title': '昭和四十年法律第三十三号 所得税法',\n",
    "    #    'url': 'https://laws.e-gov.go.jp/api/1/lawdata/340AC0000000033'\n",
    "    #},\n",
    "    #{\n",
    "    #    'title': '昭和二十八年法律第六号 酒税法',\n",
    "    #    'url': 'https://laws.e-gov.go.jp/api/1/lawdata/328AC0000000006'\n",
    "    #},        \n",
    "]\n",
    "\"\"\"\n",
    "# 429: trial token rate limit exceeded, limit is 100000 tokens per minute\n",
    "\n",
    "documents = [\n",
    "    #{\n",
    "    #    'title': 'タックスアンサーコード一覧',\n",
    "    #    'url': 'https://www.nta.go.jp/taxes/shiraberu/taxanswer/code/index.htm'\n",
    "    #},\n",
    "    {\n",
    "        'title': '所得税のしくみ',\n",
    "        'url': 'https://www.nta.go.jp/taxes/shiraberu/taxanswer/shotoku/1000.htm'\n",
    "    },\n",
    "    {\n",
    "        'title': '消費税の基本的なしくみ',\n",
    "        'url': 'https://www.nta.go.jp/taxes/shiraberu/taxanswer/shohi/6101.htm'\n",
    "    },\n",
    "    {\n",
    "        'title': '相続税がかかる場合',\n",
    "        'url': 'https://www.nta.go.jp/taxes/shiraberu/taxanswer/sozoku/4102.htm'\n",
    "    },\n",
    "    {\n",
    "        'title': 'ふるさと納税(寄附金控除)',\n",
    "        'url': 'https://www.nta.go.jp/taxes/shiraberu/taxanswer/shotoku/1155.htm'\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From LLMU\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.xml import partition_xml\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "class Vectorstore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents indexed into a vectorstore.\n",
    "\n",
    "    Parameters:\n",
    "    raw_documents (list): A list of dictionaries representing the sources of the raw documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    raw_documents (list): A list of dictionaries representing the raw documents.\n",
    "    docs (list): A list of dictionaries representing the chunked documents, with 'title', 'text', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the document chunks.\n",
    "    docs_len (int): The number of document chunks in the collection.\n",
    "    idx (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the document chunks using the Cohere API.\n",
    "    index(): Indexes the document chunks for efficient retrieval.\n",
    "    retrieve(): Retrieves document chunks based on the given query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents\n",
    "        self.docs = []\n",
    "        self.docs_embs = []\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()\n",
    "        self.embed()\n",
    "        self.index()\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for raw_document in self.raw_documents:\n",
    "            if raw_document['url'].endswith('.htm') or raw_document['url'].endswith('.html'):\n",
    "                elements = partition_html(url=raw_document['url'])\n",
    "            else:        \n",
    "                filename = f\"{raw_document['title']}.xml\"\n",
    "                with open(filename, \"w\") as f:\n",
    "                    res = requests.get(raw_document[\"url\"])\n",
    "                    f.write(res.text)\n",
    "            \n",
    "                elements = partition_xml(filename=filename)\n",
    "            chunks = chunk_by_title(elements)\n",
    "            for chunk in chunks:\n",
    "                self.docs.append(\n",
    "                    {\n",
    "                        \"title\": raw_document[\"title\"],\n",
    "                        \"text\": str(chunk),\n",
    "                        \"url\": raw_document[\"url\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.docs_len = len(self.docs)\n",
    "        for i in range(0, self.docs_len, batch_size):\n",
    "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            docs_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-v4.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.docs_embs.extend(docs_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing document chunks...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=len(self.docs_embs[0]))\n",
    "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieves document chunks based on the given query.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The query to retrieve document chunks for.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dense retrieval\n",
    "        query_emb = co.embed(\n",
    "            texts=[query], model=\"embed-v4.0\", input_type=\"search_query\"\n",
    "        ).embeddings\n",
    "        \n",
    "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # Reranking\n",
    "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
    "\n",
    "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=docs_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v3.0\",\n",
    "            rank_fields=rank_fields\n",
    "        )\n",
    "\n",
    "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
    "\n",
    "        docs_retrieved = []\n",
    "        for doc_id in doc_ids_reranked:\n",
    "            docs_retrieved.append(\n",
    "                {\n",
    "                    \"title\": self.docs[doc_id][\"title\"],\n",
    "                    \"text\": self.docs[doc_id][\"text\"],\n",
    "                    \"url\": self.docs[doc_id][\"url\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return docs_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Embedding document chunks...\n",
      "Indexing document chunks...\n",
      "Indexing complete with 107 document chunks.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Vectorstore(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = '税について何種類知っている？'\n",
    "\n",
    "response = co.chat_stream(\n",
    "    message=message,\n",
    "    model=\"command-a-03-2025\",\n",
    "    documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は、以下の税について知っています。\n",
      "- 所得税\n",
      "- 消費税\n",
      "- 相続税"
     ]
    }
   ],
   "source": [
    "for event in response:\n",
    "    if event.event_type == \"text-generation\":\n",
    "        print(event.text, end=\"\")                 # V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ddaf9-55b3-4c4c-bbda-d521e81c9119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "(まだ途中)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

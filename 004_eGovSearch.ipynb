{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ãƒ“ã‚¸ãƒã‚¹é–¢é€£æ³•æ¡ˆã® RAG æ¤œç´¢ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import cohere\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "co = cohere.ClientV2(os.environ['COHERE_APIKEY'])\n",
    "\n",
    "EGOV_SEARCH_URL = \"https://elaws.e-gov.go.jp/api/1/lawlists/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 2. e-Gov ã‹ã‚‰æ³•ä»¤æƒ…å ±ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        'title': 'æ˜­å’Œå…­åä¸‰å¹´æ³•å¾‹ç¬¬ç™¾å…«å· æ¶ˆè²»ç¨æ³•',\n",
    "        'url': 'https://laws.e-gov.go.jp/api/1/lawdata/363AC0000000108', #https://laws.e-gov.go.jp/law/363AC0000000108'\n",
    "    },\n",
    "    #{\n",
    "    #    'title': 'æ˜­å’Œå››åå¹´æ³•å¾‹ç¬¬ä¸‰åä¸‰å· æ‰€å¾—ç¨æ³•',\n",
    "    #    'url': 'https://laws.e-gov.go.jp/api/1/lawdata/340AC0000000033'\n",
    "    #},\n",
    "    #{\n",
    "    #    'title': 'æ˜­å’ŒäºŒåå…«å¹´æ³•å¾‹ç¬¬å…­å· é…’ç¨æ³•',\n",
    "    #    'url': 'https://laws.e-gov.go.jp/api/1/lawdata/328AC0000000006'\n",
    "    #},        \n",
    "]\n",
    "# 429: trial token rate limit exceeded, limit is 100000 tokens per minute\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "        'title': 'å›½ç¨åº æ¶ˆè²»ç¨ã®ã—ãã¿',\n",
    "        'url': 'https://www.nta.go.jp/publication/pamph/koho/kurashi/html/01_3.htm'\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From LLMU\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.xml import partition_xml\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "class Vectorstore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents indexed into a vectorstore.\n",
    "\n",
    "    Parameters:\n",
    "    raw_documents (list): A list of dictionaries representing the sources of the raw documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    raw_documents (list): A list of dictionaries representing the raw documents.\n",
    "    docs (list): A list of dictionaries representing the chunked documents, with 'title', 'text', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the document chunks.\n",
    "    docs_len (int): The number of document chunks in the collection.\n",
    "    idx (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the document chunks using the Cohere API.\n",
    "    index(): Indexes the document chunks for efficient retrieval.\n",
    "    retrieve(): Retrieves document chunks based on the given query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents\n",
    "        self.docs = []\n",
    "        self.docs_embs = []\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()\n",
    "        self.embed()\n",
    "        self.index()\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for raw_document in self.raw_documents:\n",
    "            if raw_document['url'].endswith('.htm') or raw_document['url'].endswith('.html'):\n",
    "                elements = partition_html(url=raw_document['url'])\n",
    "            else:        \n",
    "                filename = f\"{raw_document['title']}.xml\"\n",
    "                with open(filename, \"w\") as f:\n",
    "                    res = requests.get(raw_document[\"url\"])\n",
    "                    f.write(res.text)\n",
    "            \n",
    "                elements = partition_xml(filename=filename)\n",
    "            chunks = chunk_by_title(elements)\n",
    "            for chunk in chunks:\n",
    "                self.docs.append(\n",
    "                    {\n",
    "                        \"title\": raw_document[\"title\"],\n",
    "                        \"text\": str(chunk),\n",
    "                        \"url\": raw_document[\"url\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.docs_len = len(self.docs)\n",
    "        for i in range(0, self.docs_len, batch_size):\n",
    "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            docs_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-v4.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.docs_embs.extend(docs_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing document chunks...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
    "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
    "        print(list(range(len(self.docs_embs))))\n",
    "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieves document chunks based on the given query.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The query to retrieve document chunks for.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dense retrieval\n",
    "        query_emb = co.embed(\n",
    "            texts=[query], model=\"embed-v4.0\", input_type=\"search_query\"\n",
    "        ).embeddings\n",
    "        \n",
    "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # Reranking\n",
    "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
    "\n",
    "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=docs_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v3.0\",\n",
    "            rank_fields=rank_fields\n",
    "        )\n",
    "\n",
    "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
    "\n",
    "        docs_retrieved = []\n",
    "        for doc_id in doc_ids_reranked:\n",
    "            docs_retrieved.append(\n",
    "                {\n",
    "                    \"title\": self.docs[doc_id][\"title\"],\n",
    "                    \"text\": self.docs[doc_id][\"text\"],\n",
    "                    \"url\": self.docs[doc_id][\"url\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return docs_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Vectorstore(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'å…¬å‹™å“¡ã®ç¾©å‹™ã¯ã©ã‚Œã«è©²å½“ã™ã‚‹ï¼Ÿ'\n",
    "\n",
    "response = co.chat_stream(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message,\n",
    "        }],\n",
    "    model=\"command-a-03-2025\",\n",
    "    documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in response:\n",
    "    if event.type == \"content-delta\":\n",
    "        print(event.delta.message.content.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts: List[str], batch_size: int = 96) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’Cohere APIã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "    \n",
    "    Args:\n",
    "        texts: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    \n",
    "    Returns:\n",
    "        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®é…åˆ—\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            response = co.embed(\n",
    "                texts=batch,\n",
    "                model='embed-multilingual-v3.0',\n",
    "                input_type='search_document'\n",
    "            )\n",
    "            #logging.warning(response.embeddings)\n",
    "            embeddings.extend(response.embeddings)\n",
    "            print(f\"âœ“ {i+len(batch)}/{len(texts)} ä»¶å‡¦ç†å®Œäº†\")\n",
    "            time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–\n",
    "        except Exception as e:\n",
    "            print(f\"âš  ã‚¨ãƒ©ãƒ¼ (batch {i}): {e}\")\n",
    "            break\n",
    "    return np.array(embeddings, dtype=object)\n",
    "\n",
    "# æ³•ä»¤åã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "print(\"\\næ³•ä»¤æƒ…å ±ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...\")\n",
    "law_texts = df_laws['text'].tolist()\n",
    "law_embeddings = create_embeddings(law_texts)\n",
    "\n",
    "print(f\"âœ“ ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: {law_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. RAGæ¤œç´¢é–¢æ•°ã®å®Ÿè£…\n",
    "# =============================================================================\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œ\n",
    "    \n",
    "    Args:\n",
    "        query: æ¤œç´¢ã‚¯ã‚¨ãƒª\n",
    "        top_k: ä¸Šä½ä½•ä»¶ã‚’è¿”ã™ã‹\n",
    "    \n",
    "    Returns:\n",
    "        æ¤œç´¢çµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "    # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "    query_response = co.embed(\n",
    "        texts=[query],\n",
    "        model='embed-multilingual-v3.0',\n",
    "        input_type='search_query'\n",
    "    )\n",
    "    query_embedding = np.array(query_response.embeddings[\"float\"][0])\n",
    "    \n",
    "    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "    similarities = np.dot(law_embeddings, query_embedding) / (\n",
    "        np.linalg.norm(law_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "    )\n",
    "    \n",
    "    # ä¸Šä½kä»¶ã‚’å–å¾—\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = df_laws.iloc[top_indices].copy()\n",
    "    results['similarity_score'] = similarities[top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def rerank_results(query: str, results: pd.DataFrame, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cohere Rerank APIã§çµæœã‚’å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "    \n",
    "    Args:\n",
    "        query: æ¤œç´¢ã‚¯ã‚¨ãƒª\n",
    "        results: åˆæœŸæ¤œç´¢çµæœ\n",
    "        top_k: æœ€çµ‚çš„ã«è¿”ã™ä»¶æ•°\n",
    "    \n",
    "    Returns:\n",
    "        å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¾Œã®çµæœ\n",
    "    \"\"\"\n",
    "    documents = results['text'].tolist()\n",
    "    \n",
    "    try:\n",
    "        rerank_response = co.rerank(\n",
    "            query=query,\n",
    "            documents=documents,\n",
    "            model='rerank-multilingual-v3.0',\n",
    "            top_n=top_k\n",
    "        )\n",
    "        \n",
    "        # å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‚’æ•´ç†\n",
    "        reranked_indices = [r.index for r in rerank_response.results]\n",
    "        reranked_scores = [r.relevance_score for r in rerank_response.results]\n",
    "        \n",
    "        reranked_results = results.iloc[reranked_indices].copy()\n",
    "        reranked_results['rerank_score'] = reranked_scores\n",
    "        \n",
    "        return reranked_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš  Rerank ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return results.head(top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_queries = [\n",
    "    \"ä¼šç¤¾è¨­ç«‹ã‚„æ ªå¼ä¼šç¤¾ã®é‹å–¶ã«é–¢ã™ã‚‹æ³•å¾‹\",\n",
    "    \"å¾“æ¥­å“¡ã®é›‡ç”¨ã‚„åŠ´åƒæ¡ä»¶ã«é–¢ã™ã‚‹æ³•å¾‹\",\n",
    "    \"å€‹äººæƒ…å ±ã‚„ãƒ‡ãƒ¼ã‚¿ä¿è­·ã«é–¢ã™ã‚‹æ³•å¾‹\",\n",
    "    \"å¥‘ç´„ã‚„å–å¼•ã«é–¢ã™ã‚‹æ³•å¾‹\",\n",
    "    \"çŸ¥çš„è²¡ç”£æ¨©ã«é–¢ã™ã‚‹æ³•å¾‹\"\n",
    "]\n",
    "\n",
    "all_business_laws = []\n",
    "\n",
    "for query in business_queries:\n",
    "    print(f\"\\nğŸ” æ¤œç´¢: {query}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢\n",
    "    search_results = semantic_search(query, top_k=10)\n",
    "    \"\"\"\n",
    "    # Rerankã§ç²¾åº¦å‘ä¸Š\n",
    "    final_results = rerank_results(query, search_results, top_k=3)\n",
    "    \n",
    "    # çµæœã‚’è¡¨ç¤º\n",
    "    for idx, row in final_results.iterrows():\n",
    "        print(f\"  â€¢ {row['law_name']}\")\n",
    "        print(f\"    æ³•ä»¤ç•ªå·: {row['law_number']}\")\n",
    "        print(f\"    é–¢é€£åº¦: {row.get('rerank_score', row.get('similarity_score', 0)):.3f}\")\n",
    "        print()\n",
    "        \n",
    "        all_business_laws.append({\n",
    "            'query': query,\n",
    "            'law_name': row['law_name'],\n",
    "            'law_number': row['law_number'],\n",
    "            'score': row.get('rerank_score', row.get('similarity_score', 0))\n",
    "        })\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "(ã¾ã é€”ä¸­)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
